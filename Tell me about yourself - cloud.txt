I have over 6 years+ of experience in infrastructure designing and cloud engineering. 

I started my journey in AWS as a solutions Architect creating users and managing accounts doubling also as an AWS Cloud Engineer at Clinton Health Access Initiative Inc.
 It has been an interesting journey as I have had the opportunity to consult with different clients.I have help multiple client design multi layer architecture that are self healing and highly available following AWS best practices of security,  cost optimization,  operation excellence, reliability and performance efficiency.  Throughout my experience I have not only designed this architecture but being able to provision them manually on the console as well as using different IAC tools like cloud formation, terraform and ansible using ansible tower where I write my playbooks and use ansible tower to orchestrate to the servers in my infrastructure. I have been involved in so many projects around infrastructure build using tools like cloud formation and terraform, projects in configuration management using Ansible. I worked on an Ansible tower project for my current client involving the automation of ec2 creation.---

 I am also familiar with AWS native tools like SSM and its great components for configuration management. I have used monitoring tools like cloudwatch, Cloud Trial & AWS Config, splunk.
 
I have helped clients implement an AWS landing zone, using the AWS control Tower making sure that there are preventive and detective guardrails in place to harden the environment, making it compliant   to   AWS   best   practices   which   surrounds   a   well   architected environment.
 
I have been involved in technical writing, created solution definition documents as well as high level design documents to provision infrastructures for different environments (production, pre-prod, Dev).

 I have worked with clients who are in the process of migrating to the cloud from on premise or other cloud platforms and clients who have migrated and are in the process of expanding their footprints in the cloud.

 I   have   facilitated   the   different   phases   of   the   migration   process   which involves the  assess, mobilize, migrate, and modernize phases depending on the type of migration.

 I have also optimized cost by recommending the right storage class in S3. in my current job where the data access pattern was known, I leveraged life cycle policy to automate the movement of objects around different storage tiers, defining how long these objects could sit in one tier. 
,I had a project with a client who had just migrated to the cloud and looking for ways to expand their footprints in an AWS environment and the data access pattern was not yet know. So, I recommended S3 intelligent tiering which automatically moves data to the   most   cost effective   access   tier   without   performance   impact   or operational overhead. Still on cost optimization, I have leverage AWS  cost optimizer   to   scale   resources,   and   instance   scheduler   to   automate   the stopping and starting of EC2 instances with the use of lambda functions. I used lambda intergrated with cloud watch and cloud trail to automate mandatory tagging of resources in an environment.I have also used service catalog to help manage how resources are used in an environment. 

So instead of developers going into an environment and creating resources by themselves, we pre provision these resources using the service catalog such as Databases, and EC2 instances. I have architected infrastructures that are highly available, fault tolerant, self-healing, cost efficient and secured. On fault tolerant, when developing the solution, I put my compute instances in at least 2 availability zones, so that if one fails, we can have at least one to still have our environment. 

On self-healing, I leverage auto scaling as well as dynamic scaling policy to gain horizontal scaling around my EC2 instances especially in cases where there is fluctuating workloads. This automatically adds or subtracts the number of instances and responds to demand making it highly performant, elastic, and cost efficient. 
On high availability, I will place the EC2 instances that are hosted in at least 2 availability zones behind a load balancer to evenly distribute   traffic   to   the   instances.   Depending   on   the   data   governance requirments. when it comes to RTO and RPO (recovery time and recovery point objectives) I will introduce the DLM (Amazon data lifecycle manager)that enables backups for the EC2 instances to be taken at particular times depending on the retention policy. 

On security,
 I have leveraged network security such as SGs, At the level of my instance- to help whitelist and filter traffic based on source IPâ€™s Ports, Protocols.
 NACL,at the level of my NETWORK. access controls  with  IAM  and  bucket policies  and AWS best practices  of least privilege permission as well as encryption at rest and in transit with KMS.(key mgt service which is a secure and resilient service that uses hardware security modules that have been validated or are in the process of being validated, to protect your keys). KMS is integrated with CloudTrail to provide you with logs of all key usage to help meet your regulatory and compliance needs. On compute infrastructure for instance, I would recommend the best EC2   instance   purchase   option   as   well   as   recommend   right   sizing   the environment to optimize cost.  GUARD DUTY: For continuous security monitoring within your environment. It intelligently
detects threats and protects accounts and workloads. It analyses Flowlogs, Cloudtrail and DNS logs.
SNS for NOTIFICATION purposes. (when a rule matches an event, it triggers lambda, lambda -
SNS)



I have used Jenkins and integrated with GitHub to set up an environment for CICD.  I did the initial set up of Jenkins including installation, getting it on a web page, and connecting it to Github using Webhooks. So, every time a code   is   checked   (committed)   it   will   trigger   the   Jenkins   pipeline   to automatically check out the new code and make a build. Next, I installed sonarqube that helps to scan the code  and If the build succeeds, it will automatically deploy the code to the QA or stage environment. If the build fails, Jenkins will send a notification to the developer that checked in the code. I also had to specify the git repo in the Jenkins configuration by installing Nexus. Also, I have used Jenkins to create free style jobs where I scheduled jobs such as patching an environment or deploying CFT and other codes. The beauty with this feature over cron is that you are able to build with a condition that the second job should only be initiated when the first job is successful. Ex, I can have my CFT finish provisioning servers and if this job succeeds then my ansible play book role will run to configure the build servers. 

I have used IAC to automate the process of provisioning resources within infrastructures in my environment. When I joined Clinton Health Access Initiative, most of their resources were provisioned manually in the AWS console and some were provisioned using a monolithic template. The team I got hired into was to help in refactoring these templates into standard cloud formation templates. I helped in componentizing these templates into multiple stacks of service categories, separated templates for compute, databases and networking. Even at the level of compute, we had different templates for the app and web tiers. Following AWS coding best practices,   I   avoided   hard   cording   and   made   the   templets   dynamic   by parameterizing the resources to be provisioned. For instance with a resources such as EC2,  I parameterized the subnets, SGs and AMI's to avoid hard coding. I 
also made use of cross  stack referencing by using the output section of the template to export standard features like SGs which are then imported when provisioning resources. The parameter section of the template enables me to input custom values to my template each time I create or update a stack. These parameters can be referenced from the resource and output section of the template The resource section declares the AWS resources that I want to include in the stack.   


